# vlmrun
Run a vision language model

# package
# Install the required packages for Qwen2.5-VL
pip install transformers>=4.40.0
pip install torch torchvision torchaudio
pip install pillow requests
pip install qwen-vl-utils
pip install flash-attn --no-build-isolation  # For flash attention optimization